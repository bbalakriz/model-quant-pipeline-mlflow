{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7854d876-3cd1-4a18-bf9c-4c947166fd88",
   "metadata": {},
   "source": [
    "# Quantization of `granite-3.3-2b-instruct` model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b258eb-3223-4594-860e-daae9d3a5c1d",
   "metadata": {},
   "source": [
    "Recall that our overall solution uses the quantized version of the model `granite-3.3-2b-instruct`. In this lab, we will be taking in the base model `granite-3.3-2b-instruct` and quantizing it to `W4A16` - which is fixed-point integer (INT) quantization scheme for weights and floating‑point for activations - to provide both memory savings (weight - INT4) and inference acceleration (activations - BF16) with `vLLM`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7800f926-dc4c-42a1-84a7-03be28d7463f",
   "metadata": {},
   "source": [
    "**Note**: `W4A16` computation is supported on Nvidia GPUs with compute capability > 7.5 (Turing, Ampere, Ada Lovelace, Hopper).\n",
    "\n",
    "**Note**: The steps here will take around 20-30 minutes, depending on the connectivity. The most time consuming steps are the installation of llmcompressor (up to 5 mins) and the quantization step (which can take more anywhere between 10-15 mins)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef99081-3a8b-486c-9882-deb387c83850",
   "metadata": {},
   "source": [
    "## Setting up llm-compressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53691745",
   "metadata": {},
   "source": [
    "Installing `llmcompressor` may take a minute, depending on the bandwith available. Do note the versions of `transformer` library we would be using. There is a known issue (*torch.fx.proxy.TraceError: symbolically traced variables cannot be used as inputs to control flow*) with the usage of the latest transformer library (version `4.53.2` as of July 17, 2025) in combination with the latest version of llmcompressor (version `0.6.0`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5037b244-10f5-4aac-981a-3a07864e0e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q llmcompressor==0.6.0 transformers==4.52.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c484403-9fd6-489d-9912-8e7153c97749",
   "metadata": {},
   "source": [
    "Let's make sure we have installed the right versions installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e9e9b1-1e04-4c1c-be37-617c489b2678",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip list | grep llmcompressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e405cb5c-843d-4f12-8fbc-fa1e8db4f7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip list | grep transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd24bf14-c77b-4f07-851d-d70cc61646eb",
   "metadata": {},
   "source": [
    "## Let' start with the quantization of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55327b5b-b309-4c23-a245-70d78d0955fc",
   "metadata": {},
   "source": [
    "There are 6 steps:\n",
    "1. Loading the model\n",
    "2. Choosing the quantization scheme and method\n",
    "3. Preparing the calibration data\n",
    "4. Applying quantization\n",
    "5. Saving the model\n",
    "6. Evaluation of accuracy in vLLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aad553f-1f6f-4583-b280-6205930debe8",
   "metadata": {},
   "source": [
    "### Loading the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99299f19",
   "metadata": {},
   "source": [
    "Load the model using AutoModelForCausalLM for handling quantized saving and loading. The model can be loaded from HuggingFace directly as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ad8b7b-bb67-4d02-8cce-f80de7462f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "MODEL_ID = \"ibm-granite/granite-3.2-2b-instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID, device_map=\"auto\", torch_dtype=\"auto\",\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9742346d-f226-4462-871a-bf4008a04091",
   "metadata": {},
   "source": [
    "### Prepare calibration data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bdaadd-11b8-4627-be3d-3a1ffa4c9f88",
   "metadata": {},
   "source": [
    "Prepare the calibration data. When quantizing weigths of a model to int4 using GPTQ, we need some sample data to run the GPTQ algorithms. As a result, it is very important to use calibration data that closely matches the type of data used in our deployment. If you have fine-tuned a model, using a sample of your training data is a good idea.\n",
    "\n",
    "In our case, we are quantizing an Instruction tuned generic model, so we will use the ultrachat dataset. Some best practices include:\n",
    "- 512 samples is a good place to start (increase if accuracy drops). We are going to use 256 to speed up the process.\n",
    "- 2048 sequence length is a good place to start\n",
    "- Use the chat template or instrucion template that the model is trained with\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c3b1c6-f408-4ec5-81a0-3e8ae8019070",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "NUM_CALIBRATION_SAMPLES = 512  # 1024\n",
    "DATASET_ID = \"neuralmagic/LLM_compression_calibration\"\n",
    "DATASET_SPLIT = \"train\"\n",
    "\n",
    "# Load dataset.\n",
    "ds = load_dataset(DATASET_ID, split=DATASET_SPLIT)\n",
    "ds = ds.shuffle(seed=42).select(range(NUM_CALIBRATION_SAMPLES))\n",
    "\n",
    "# Preprocess the data into the format the model is trained with.\n",
    "def preprocess(example):\n",
    "    return {\"text\": example[\"text\"]}\n",
    "ds = ds.map(preprocess)\n",
    "\n",
    "# Tokenize the data\n",
    "def tokenize(sample):\n",
    "    return tokenizer(\n",
    "        sample[\"text\"],\n",
    "        padding=False,\n",
    "        truncation=False,\n",
    "        add_special_tokens=True,\n",
    "    )\n",
    "ds = ds.map(tokenize, remove_columns=ds.column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2c889b-b827-4f8d-be34-2cc05bba48f1",
   "metadata": {},
   "source": [
    "With the dataset ready, we will now apply quantization.\n",
    "\n",
    "We first select the quantization algorithm. For W4A16, we want to:\n",
    "- Run SmoothQuant to make the activations easier to quantize\n",
    "- Quantize the weights to 4 bits with channelwise scales using GPTQ\n",
    "- Quantize the activations with dynamic per token strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1732929",
   "metadata": {},
   "source": [
    "**Note**: The quantization step takes a long time to complete due to the callibration requirements -- around 10 - 15 mins, depending on the GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9593615d-db1d-4b80-bd94-93868a45cd01",
   "metadata": {},
   "source": [
    "### Imports and definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da32a22-49bd-4480-b0e0-b0e052190ce0",
   "metadata": {},
   "source": [
    "**GPTQModifier**: Applies Gentle Quantization (GPTQ) for weight-only quantization.\n",
    "\n",
    "**SmoothQuantModifier**: Prepares model activations for smoother quantization by scaling internal activations and weights.\n",
    "\n",
    "**oneshot**: High-level API that applies your quantization recipe in one go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c06b7f-aedb-4e06-98cd-16aed3abd046",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmcompressor.modifiers.quantization import GPTQModifier\n",
    "from llmcompressor.transformers import oneshot\n",
    "from llmcompressor.modifiers.smoothquant import SmoothQuantModifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556fec70-e27b-4bab-8e6b-f6268636c5d9",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12866bd2-e08a-419c-98c7-ced3e5853baa",
   "metadata": {},
   "source": [
    "Rationale\n",
    "- **DAMPENING_FRAC=0.1** gently prevents large Hessian-derived updates during quantization.\n",
    "- **OBSERVER=\"mse\"** measures quantization error by squared deviations, yielding well-rounded scales.\n",
    "- **GROUP_SIZE=128** determines group size for per-channel quantization; typical default usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35cf573-dc89-4b5d-9414-aad06992c2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "DAMPENING_FRAC = 0.1  # tapering adjustment to prevent extreme weight updates\n",
    "OBSERVER = \"mse\"  # denotes minmax - quantization layout based on mean‐squared‐error\n",
    "GROUP_SIZE = 128  # # per-channel grouping width for quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cb39b8-9fe3-4a7e-adba-9a03ec033344",
   "metadata": {},
   "source": [
    "### Layer Mappings & Ignoring Heads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422d76f3-61a5-474f-8b83-3005199b7122",
   "metadata": {},
   "source": [
    "Logic\n",
    "\n",
    "- **ignore=[\"lm_head\"]** skips quantization on the output layer to preserve final logits and maintain accuracy.\n",
    "- mappings link groups of linear projections (q, k, v, gating, up/down projections) with layernorm blocks—SmoothQuant uses these to shift and normalize activations across paired layers for better quant distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4604b8-b311-428f-838e-a50bfc041b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ignore=[\"lm_head\"]\n",
    "mappings=[\n",
    "    [[\"re:.*q_proj\", \"re:.*k_proj\", \"re:.*v_proj\"], \"re:.*input_layernorm\"],\n",
    "    [[\"re:.*gate_proj\", \"re:.*up_proj\"], \"re:.*post_attention_layernorm\"],\n",
    "    [[\"re:.*down_proj\"], \"re:.*up_proj\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8918c2e-d6f1-4e31-aa56-57b5a7c3f1df",
   "metadata": {},
   "source": [
    "### Recipe Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b291ad00-d881-4efb-bba5-2b20d3b28a3d",
   "metadata": {},
   "source": [
    "**Workflow**\n",
    "\n",
    "- **SmoothQuantModifier**: Re-scales activations across paired layers before quantization to reduce outliers (smoothing_strength=0.7, high smoothing but not extreme).\n",
    "- **GPTQModifier**: Performs Weight-Only quantization (4-bit weights, 16-bit activations) on all Linear layers except those ignored, applying your dampening and observer settings. Scheme \"W4A16\" reduces model size while maintaining decent accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abec4d4-a2a8-4e98-a813-b24e0aa8dc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe = [\n",
    "    SmoothQuantModifier(smoothing_strength=0.7, ignore=ignore, mappings=mappings),\n",
    "    GPTQModifier(\n",
    "        targets=[\"Linear\"],\n",
    "        ignore=ignore,\n",
    "        scheme=\"W4A16\",\n",
    "        dampening_frac=DAMPENING_FRAC,\n",
    "        observer=OBSERVER,\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8d0ba1-ca11-406b-b232-2d6edc17fcc7",
   "metadata": {},
   "source": [
    "### Quantize in One Shot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96817063-9eb7-4d4b-8ed9-19673f74967b",
   "metadata": {},
   "source": [
    "**How It Works**\n",
    "\n",
    "- Feeds dataset (calibration set) into your model to gather activation statistics.\n",
    "- Applies SmoothQuant rescaling followed by GPTQ quantization in a sequential per-layer manner.\n",
    "- **max_seq_length=8196** ensures large context coverage for calibration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bf36ae-0774-4cee-8f99-c367c24303d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "oneshot(\n",
    "    model=model,\n",
    "    dataset=ds,\n",
    "    recipe=recipe,\n",
    "    num_calibration_samples=NUM_CALIBRATION_SAMPLES,\n",
    "    max_seq_length=8196,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105eef1b-2db4-4678-b3d5-9ebc33257424",
   "metadata": {},
   "source": [
    "### Save the Compressed Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712c61fb-8493-40d5-b15c-60f18084aaab",
   "metadata": {},
   "source": [
    "**Explanation**\n",
    "\n",
    "- Naming: appends -W4A16 to distinguish the quantized checkpoint.\n",
    "- **save_compressed=True** stores weights in compact safetensors format for deployment via vLLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f6d53f-8db5-40a1-874f-14d60b007b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to disk compressed.\n",
    "SAVE_DIR = MODEL_ID.split(\"/\")[-1] + \"-W4A16\"\n",
    "model.save_pretrained(SAVE_DIR, save_compressed=True)\n",
    "tokenizer.save_pretrained(SAVE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed216fb-9c28-4bff-8033-b68c744b4cd9",
   "metadata": {},
   "source": [
    "### Evaluate accuracy in vLLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa02cc91-dfca-4dc1-b150-692e3a309ad3",
   "metadata": {},
   "source": [
    "We can evaluate accuracy with lm_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737002c6-1512-402f-adfa-965660478ece",
   "metadata": {},
   "source": [
    "##### Check GPU memory leftovers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119cf001-f52b-46a2-a034-1a5ac99d8f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e2fa79-3e05-4f83-90b7-dd7be66513dc",
   "metadata": {},
   "source": [
    "**IMPORTANT**: After quantizing the model the GPU memory may not be freed (see the above output). You need to **restart the kernel** before evaluating the model to ensure you have enough GPU RAM available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b5eab2-e7f1-4631-b74f-8e5b44d06dc2",
   "metadata": {},
   "source": [
    "#### Install lm_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a8a3ad8-effc-4212-9c5d-ab09d772e73c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q lm_eval==v0.4.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7facbbf2-400c-424f-bada-4f224d4f5fd0",
   "metadata": {},
   "source": [
    "#### Install vLLM for evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d660bb-9435-4d1c-9599-7e9f8f1ca4ff",
   "metadata": {},
   "source": [
    "Run the following to test accuracy on GSM-8K:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe802b4d-14f4-4379-a05d-e91741770c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "codeflare-sdk 0.27.0 requires pydantic<2, but you have pydantic 2.11.7 which is incompatible.\n",
      "codeflare-sdk 0.27.0 requires ray[data,default]==2.35.0, but you have ray 2.47.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -q vllm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9575f87-2dcd-4eb6-b433-9ddd56bde6f4",
   "metadata": {},
   "source": [
    "### Evaluation Command"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac46967-6933-4cad-8f11-49bb61a5dd42",
   "metadata": {},
   "source": [
    "- `--model vllm` - Uses vLLM backend for fast, memory-efficient inference on large models \n",
    "- `--model_args` - pretrained=$MODEL_ID: specifies which model to load.\n",
    "- `add_bos_token=true`: ensures a beginning-of-sequence token is added; required for consistent results on math and reasoning tasks \n",
    "- `max_model_len=4096`: sets the context window the model uses for evaluation.\n",
    "- `gpu_memory_utilization=0.5`: limits vLLM to use 50% of GPU memory, allowing to avoid OOM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a005c893-fe46-4f06-8053-a4ac9321d445",
   "metadata": {},
   "outputs": [],
   "source": [
    "!lm-eval --tasks list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "00fd85be-edf1-4736-86eb-fd93c09c146a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-22 01:34:43 [__init__.py:244] Automatically detected platform cuda.\n",
      "2025-07-22:01:34:45,244 INFO     [__main__.py:272] Verbosity set to INFO\n",
      "2025-07-22:01:34:49,202 WARNING  [__main__.py:312]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.\n",
      "2025-07-22:01:34:49,203 INFO     [__main__.py:357] Passed `--trust_remote_code`, setting environment variable `HF_DATASETS_TRUST_REMOTE_CODE=true`\n",
      "2025-07-22:01:34:49,203 INFO     [__main__.py:369] Selected Tasks: ['arc_easy', 'gsm8k']\n",
      "2025-07-22:01:34:49,205 INFO     [evaluator.py:152] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
      "2025-07-22:01:34:49,205 INFO     [evaluator.py:189] Initializing vllm model, with arguments: {'pretrained': '/opt/app-root/src/showroom-summit2025-lb2959-neural-magic/lab-materials/03/granite-3.2-2b-instruct-W4A16', 'add_bos_token': True, 'max_model_len': 4096, 'gpu_memory_utilization': 0.5, 'trust_remote_code': True}\n",
      "INFO 07-22 01:34:55 [config.py:841] This model supports multiple tasks: {'generate', 'embed', 'reward', 'classify'}. Defaulting to 'generate'.\n",
      "INFO 07-22 01:34:55 [config.py:1472] Using max model len 4096\n",
      "INFO 07-22 01:34:55 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 07-22 01:34:55 [core.py:526] Waiting for init message from front-end.\n",
      "INFO 07-22 01:34:55 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/opt/app-root/src/showroom-summit2025-lb2959-neural-magic/lab-materials/03/granite-3.2-2b-instruct-W4A16', speculative_config=None, tokenizer='/opt/app-root/src/showroom-summit2025-lb2959-neural-magic/lab-materials/03/granite-3.2-2b-instruct-W4A16', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=1234, served_model_name=/opt/app-root/src/showroom-summit2025-lb2959-neural-magic/lab-materials/03/granite-3.2-2b-instruct-W4A16, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "INFO 07-22 01:34:56 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "WARNING 07-22 01:34:56 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 07-22 01:34:56 [gpu_model_runner.py:1770] Starting to load model /opt/app-root/src/showroom-summit2025-lb2959-neural-magic/lab-materials/03/granite-3.2-2b-instruct-W4A16...\n",
      "INFO 07-22 01:34:56 [gpu_model_runner.py:1775] Loading model from scratch...\n",
      "INFO 07-22 01:34:56 [compressed_tensors_wNa16.py:95] Using MarlinLinearKernel for CompressedTensorsWNA16\n",
      "INFO 07-22 01:34:56 [cuda.py:284] Using Flash Attention backend on V1 engine.\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  4.11it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  4.11it/s]\n",
      "\n",
      "INFO 07-22 01:34:57 [default_loader.py:272] Loading weights took 0.29 seconds\n",
      "INFO 07-22 01:34:57 [gpu_model_runner.py:1801] Model loading took 1.4034 GiB and 0.555227 seconds\n",
      "INFO 07-22 01:35:08 [backends.py:508] Using cache directory: /opt/app-root/src/.cache/vllm/torch_compile_cache/59e878adac/rank_0_0/backbone for vLLM's torch.compile\n",
      "INFO 07-22 01:35:08 [backends.py:519] Dynamo bytecode transform time: 10.11 s\n",
      "INFO 07-22 01:35:15 [backends.py:155] Directly load the compiled graph(s) for shape None from the cache, took 5.841 s\n",
      "INFO 07-22 01:35:17 [monitor.py:34] torch.compile takes 10.11 s in total\n",
      "INFO 07-22 01:35:18 [gpu_worker.py:232] Available KV cache memory: 9.07 GiB\n",
      "INFO 07-22 01:35:19 [kv_cache_utils.py:716] GPU KV cache size: 118,896 tokens\n",
      "INFO 07-22 01:35:19 [kv_cache_utils.py:720] Maximum concurrency for 4,096 tokens per request: 29.03x\n",
      "Capturing CUDA graph shapes: 100%|██████████████| 67/67 [00:26<00:00,  2.53it/s]\n",
      "INFO 07-22 01:35:45 [gpu_model_runner.py:2326] Graph capturing finished in 27 secs, took 0.64 GiB\n",
      "INFO 07-22 01:35:45 [core.py:172] init engine (profile, create kv cache, warmup model) took 47.72 seconds\n",
      "2025-07-22:01:35:48,714 WARNING  [evaluator.py:251] Overwriting default num_fewshot of gsm8k from 5 to 5\n",
      "2025-07-22:01:35:48,714 INFO     [evaluator.py:261] Setting fewshot random generator seed to 1234\n",
      "2025-07-22:01:35:48,714 WARNING  [evaluator.py:251] Overwriting default num_fewshot of arc_easy from None to 5\n",
      "2025-07-22:01:35:48,714 INFO     [evaluator.py:261] Setting fewshot random generator seed to 1234\n",
      "2025-07-22:01:35:48,715 INFO     [task.py:411] Building contexts for gsm8k on rank 0...\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 342.95it/s]\n",
      "2025-07-22:01:35:48,863 INFO     [task.py:411] Building contexts for arc_easy on rank 0...\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 175.00it/s]\n",
      "2025-07-22:01:35:49,151 INFO     [evaluator.py:438] Running generate_until requests\n",
      "Running generate_until requests:   0%|                   | 0/50 [00:00<?, ?it/s]\n",
      "Adding requests: 100%|████████████████████████| 50/50 [00:00<00:00, 7028.46it/s]\u001b[A\n",
      "\n",
      "Processed prompts:   0%| | 0/50 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, \u001b[A\n",
      "Processed prompts:   2%| | 1/50 [00:08<06:51,  8.39s/it, est. speed input: 138.8\u001b[A\n",
      "Processed prompts:   4%| | 2/50 [00:08<02:50,  3.55s/it, est. speed input: 273.4\u001b[A\n",
      "Processed prompts:   8%| | 4/50 [00:08<01:04,  1.41s/it, est. speed input: 513.6\u001b[A\n",
      "Processed prompts:  10%| | 5/50 [00:08<00:45,  1.01s/it, est. speed input: 623.7\u001b[A\n",
      "Processed prompts:  12%| | 6/50 [00:09<00:33,  1.33it/s, est. speed input: 745.5\u001b[A\n",
      "Processed prompts:  14%|▏| 7/50 [00:09<00:24,  1.79it/s, est. speed input: 898.8\u001b[A\n",
      "Processed prompts:  18%|▏| 9/50 [00:09<00:15,  2.60it/s, est. speed input: 1112.\u001b[A\n",
      "Processed prompts:  22%|▏| 11/50 [00:09<00:10,  3.60it/s, est. speed input: 1318\u001b[A\n",
      "Processed prompts:  26%|▎| 13/50 [00:10<00:07,  4.68it/s, est. speed input: 1513\u001b[A\n",
      "Processed prompts:  30%|▎| 15/50 [00:10<00:05,  6.24it/s, est. speed input: 1675\u001b[A\n",
      "Processed prompts:  38%|▍| 19/50 [00:10<00:03,  9.91it/s, est. speed input: 2010\u001b[A\n",
      "Processed prompts:  42%|▍| 21/50 [00:10<00:02, 11.04it/s, est. speed input: 2204\u001b[A\n",
      "Processed prompts:  46%|▍| 23/50 [00:10<00:02,  9.55it/s, est. speed input: 2342\u001b[A\n",
      "Processed prompts:  50%|▌| 25/50 [00:10<00:02, 10.90it/s, est. speed input: 2513\u001b[A\n",
      "Processed prompts:  54%|▌| 27/50 [00:11<00:02,  9.96it/s, est. speed input: 2640\u001b[A\n",
      "Processed prompts:  58%|▌| 29/50 [00:11<00:01, 11.50it/s, est. speed input: 2833\u001b[A\n",
      "Processed prompts:  64%|▋| 32/50 [00:11<00:01, 13.97it/s, est. speed input: 3109\u001b[A\n",
      "Processed prompts:  68%|▋| 34/50 [00:11<00:01, 14.83it/s, est. speed input: 3265\u001b[A\n",
      "Processed prompts:  72%|▋| 36/50 [00:11<00:01, 13.15it/s, est. speed input: 3438\u001b[A\n",
      "Processed prompts:  78%|▊| 39/50 [00:11<00:00, 15.61it/s, est. speed input: 3682\u001b[A\n",
      "Processed prompts:  86%|▊| 43/50 [00:11<00:00, 19.20it/s, est. speed input: 4035\u001b[A\n",
      "Processed prompts:  92%|▉| 46/50 [00:12<00:00, 12.12it/s, est. speed input: 4157\u001b[A\n",
      "Processed prompts: 100%|█| 50/50 [00:12<00:00,  4.04it/s, est. speed input: 4485\u001b[A\n",
      "Running generate_until requests: 100%|██████████| 50/50 [00:12<00:00,  4.03it/s]\n",
      "2025-07-22:01:36:01,610 INFO     [evaluator.py:438] Running loglikelihood requests\n",
      "Running loglikelihood requests:   0%|                   | 0/200 [00:00<?, ?it/s]\n",
      "Adding requests: 100%|██████████████████████| 200/200 [00:00<00:00, 9956.10it/s]\u001b[A\n",
      "\n",
      "Processed prompts:   0%| | 0/200 [00:00<?, ?it/s, est. speed input: 0.00 toks/s,\u001b[A\n",
      "Processed prompts:   2%| | 3/200 [00:01<01:27,  2.26it/s, est. speed input: 759.\u001b[A\n",
      "Processed prompts:  16%|▏| 31/200 [00:02<00:09, 18.00it/s, est. speed input: 436\u001b[A\n",
      "Processed prompts:  31%|▎| 62/200 [00:03<00:05, 24.13it/s, est. speed input: 560\u001b[A\n",
      "Processed prompts:  48%|▍| 96/200 [00:04<00:03, 28.21it/s, est. speed input: 629\u001b[A\n",
      "Processed prompts:  66%|▋| 133/200 [00:05<00:02, 31.62it/s, est. speed input: 66\u001b[A\n",
      "Processed prompts:  88%|▉| 176/200 [00:05<00:00, 43.92it/s, est. speed input: 76\u001b[A\n",
      "Processed prompts: 100%|█| 200/200 [00:05<00:00, 36.55it/s, est. speed input: 83\u001b[A\n",
      "Running loglikelihood requests: 100%|█████████| 200/200 [00:05<00:00, 36.28it/s]\n",
      "2025-07-22:01:36:08,248 INFO     [evaluation_tracker.py:182] Saving results aggregated\n",
      "vllm (pretrained=/opt/app-root/src/showroom-summit2025-lb2959-neural-magic/lab-materials/03/granite-3.2-2b-instruct-W4A16,add_bos_token=true,max_model_len=4096,gpu_memory_utilization=0.5,trust_remote_code=True), gen_kwargs: (None), limit: 50.0, num_fewshot: 5, batch_size: auto\n",
      "| Tasks  |Version|     Filter     |n-shot|  Metric   |   |Value|   |Stderr|\n",
      "|--------|------:|----------------|-----:|-----------|---|----:|---|-----:|\n",
      "|arc_easy|      1|none            |     5|acc        |↑  | 0.70|±  |0.0655|\n",
      "|        |       |none            |     5|acc_norm   |↑  | 0.78|±  |0.0592|\n",
      "|gsm8k   |      3|flexible-extract|     5|exact_match|↑  | 0.60|±  |0.0700|\n",
      "|        |       |strict-match    |     5|exact_match|↑  | 0.48|±  |0.0714|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "MODEL_ID = current_dir + \"/granite-3.2-2b-instruct-W4A16\"\n",
    "\n",
    "!lm_eval --model vllm \\\n",
    "  --model_args \"pretrained=$MODEL_ID,add_bos_token=true,max_model_len=4096,gpu_memory_utilization=0.5\" \\\n",
    "  --trust_remote_code \\\n",
    "  --tasks gsm8k,arc_easy \\\n",
    "  --num_fewshot 5 \\\n",
    "  --limit 50 \\\n",
    "  --batch_size 'auto'\\\n",
    "  --output_path \"/tmp/results_json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "72d851ba-16ed-4cb7-8fd7-d5ed55bb6633",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, shutil, os\n",
    "\n",
    "folder = \"/tmp/results_json\"\n",
    "os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "# Find the eval file\n",
    "matches = glob.glob(os.path.join(folder, \"**\", \"*.json\"), recursive=True)\n",
    "if not matches:\n",
    "    raise RuntimeError(\"No JSON found\")\n",
    "src = matches[0]\n",
    "dst = os.path.join(\"./results_json\", \"results.json\")\n",
    "shutil.move(src, dst)\n",
    "results_json = dst"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b570baf-46e1-46e9-b970-c3f156814a69",
   "metadata": {},
   "source": [
    "With powerful GPU(s), you could also run the vLLM based evals with the following - using higher GPU memory utilization and chunked prefill. \n",
    "```bash\n",
    "!lm_eval \\\n",
    "  --model vllm \\\n",
    "  --model_args pretrained=$SAVE_DIR,dtype=auto,add_bos_token=True,max_model_len=4096,tensor_parallel_size=1,gpu_memory_utilization=0.8,enable_chunked_prefill=True \\\n",
    "  --trust_remote_code \\\n",
    "  --tasks openllm \\\n",
    "  --write_out \\\n",
    "  --batch_size auto \\\n",
    "  --output_path output_dir \\\n",
    "  --show_config\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2aa2928-327f-4e81-90be-82623d3bd8a7",
   "metadata": {},
   "source": [
    "**Next Steps**: \n",
    "- How would you futher improve the accuacy of the model?\n",
    "- How would you go about preparing the right data set for a different use case?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1a1582",
   "metadata": {},
   "source": [
    "### Optionally, upload the optimized model to MinIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb1e422-0ea3-41df-97cd-36cbcf4e5520",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from boto3 import client\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "OPTIMIZED_MODEL_DIR = current_dir + \"/granite-3.2-2b-instruct-W4A16\"\n",
    "S3_PATH = \"granite-int4-notebook\"\n",
    "\n",
    "print('Starting upload of quantizied model')\n",
    "s3_endpoint_url = os.environ[\"AWS_S3_ENDPOINT\"]\n",
    "s3_access_key = os.environ[\"AWS_ACCESS_KEY_ID\"]\n",
    "s3_secret_key = os.environ[\"AWS_SECRET_ACCESS_KEY\"]\n",
    "s3_bucket_name = os.environ[\"AWS_S3_BUCKET\"]\n",
    "\n",
    "print(f'Uploading predictions to bucket {s3_bucket_name} '\n",
    "        f'to S3 storage at {s3_endpoint_url}')\n",
    "\n",
    "s3_client = client(\n",
    "    's3', endpoint_url=s3_endpoint_url, aws_access_key_id=s3_access_key,\n",
    "    aws_secret_access_key=s3_secret_key, verify=False\n",
    ")\n",
    "\n",
    "# Walk through the local folder and upload files\n",
    "for root, dirs, files in os.walk(OPTIMIZED_MODEL_DIR):\n",
    "    for file in files:\n",
    "        local_file_path = os.path.join(root, file)\n",
    "        s3_file_path = os.path.join(S3_PATH, local_file_path[len(OPTIMIZED_MODEL_DIR)+1:])\n",
    "        s3_client.upload_file(local_file_path, s3_bucket_name, s3_file_path)\n",
    "        print(f'Uploaded {local_file_path}')\n",
    "\n",
    "print('Finished uploading of quantizied model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d715a469-87a0-4760-a520-63abc3f48431",
   "metadata": {},
   "source": [
    "### Push the lm-eval metrics to mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98914784-b0c3-4f0d-8216-53ed924450d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mlflow==2.16\n",
      "  Downloading mlflow-2.16.0-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting mlflow-skinny==2.16.0 (from mlflow==2.16)\n",
      "  Downloading mlflow_skinny-2.16.0-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting Flask<4 (from mlflow==2.16)\n",
      "  Downloading flask-3.1.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting alembic!=1.10.0,<2 (from mlflow==2.16)\n",
      "  Downloading alembic-1.16.4-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting docker<8,>=4.0.0 (from mlflow==2.16)\n",
      "  Downloading docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting graphene<4 (from mlflow==2.16)\n",
      "  Downloading graphene-3.4.3-py2.py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting markdown<4,>=3.3 (from mlflow==2.16)\n",
      "  Downloading markdown-3.8.2-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: matplotlib<4 in /opt/app-root/lib64/python3.11/site-packages (from mlflow==2.16) (3.10.1)\n",
      "Requirement already satisfied: numpy<3 in /opt/app-root/lib64/python3.11/site-packages (from mlflow==2.16) (2.2.3)\n",
      "Requirement already satisfied: pandas<3 in /opt/app-root/lib64/python3.11/site-packages (from mlflow==2.16) (2.2.3)\n",
      "Collecting pyarrow<18,>=4.0.0 (from mlflow==2.16)\n",
      "  Downloading pyarrow-17.0.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: scikit-learn<2 in /opt/app-root/lib64/python3.11/site-packages (from mlflow==2.16) (1.6.1)\n",
      "Requirement already satisfied: scipy<2 in /opt/app-root/lib64/python3.11/site-packages (from mlflow==2.16) (1.15.2)\n",
      "Collecting sqlalchemy<3,>=1.4.0 (from mlflow==2.16)\n",
      "  Downloading sqlalchemy-2.0.41-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
      "Requirement already satisfied: Jinja2<4,>=2.11 in /opt/app-root/lib64/python3.11/site-packages (from mlflow==2.16) (3.1.6)\n",
      "Collecting gunicorn<24 (from mlflow==2.16)\n",
      "  Downloading gunicorn-23.0.0-py3-none-any.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: cachetools<6,>=5.0.0 in /opt/app-root/lib64/python3.11/site-packages (from mlflow-skinny==2.16.0->mlflow==2.16) (5.5.2)\n",
      "Requirement already satisfied: click<9,>=7.0 in /opt/app-root/lib64/python3.11/site-packages (from mlflow-skinny==2.16.0->mlflow==2.16) (8.1.8)\n",
      "Requirement already satisfied: cloudpickle<4 in /opt/app-root/lib64/python3.11/site-packages (from mlflow-skinny==2.16.0->mlflow==2.16) (3.1.1)\n",
      "Collecting databricks-sdk<1,>=0.20.0 (from mlflow-skinny==2.16.0->mlflow==2.16)\n",
      "  Downloading databricks_sdk-0.59.0-py3-none-any.whl.metadata (39 kB)\n",
      "Requirement already satisfied: gitpython<4,>=3.1.9 in /opt/app-root/lib64/python3.11/site-packages (from mlflow-skinny==2.16.0->mlflow==2.16) (3.1.44)\n",
      "Collecting importlib-metadata!=4.7.0,<9,>=3.7.0 (from mlflow-skinny==2.16.0->mlflow==2.16)\n",
      "  Downloading importlib_metadata-8.7.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting opentelemetry-api<3,>=1.9.0 (from mlflow-skinny==2.16.0->mlflow==2.16)\n",
      "  Downloading opentelemetry_api-1.35.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting opentelemetry-sdk<3,>=1.9.0 (from mlflow-skinny==2.16.0->mlflow==2.16)\n",
      "  Downloading opentelemetry_sdk-1.35.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: packaging<25 in /opt/app-root/lib64/python3.11/site-packages (from mlflow-skinny==2.16.0->mlflow==2.16) (24.2)\n",
      "Requirement already satisfied: protobuf<6,>=3.12.0 in /opt/app-root/lib64/python3.11/site-packages (from mlflow-skinny==2.16.0->mlflow==2.16) (4.25.6)\n",
      "Requirement already satisfied: pyyaml<7,>=5.1 in /opt/app-root/lib64/python3.11/site-packages (from mlflow-skinny==2.16.0->mlflow==2.16) (6.0.2)\n",
      "Requirement already satisfied: requests<3,>=2.17.3 in /opt/app-root/lib64/python3.11/site-packages (from mlflow-skinny==2.16.0->mlflow==2.16) (2.32.3)\n",
      "Collecting sqlparse<1,>=0.4.0 (from mlflow-skinny==2.16.0->mlflow==2.16)\n",
      "  Downloading sqlparse-0.5.3-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting Mako (from alembic!=1.10.0,<2->mlflow==2.16)\n",
      "  Downloading mako-1.3.10-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.12 in /opt/app-root/lib64/python3.11/site-packages (from alembic!=1.10.0,<2->mlflow==2.16) (4.12.2)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /opt/app-root/lib64/python3.11/site-packages (from docker<8,>=4.0.0->mlflow==2.16) (1.26.20)\n",
      "Collecting blinker>=1.9.0 (from Flask<4->mlflow==2.16)\n",
      "  Downloading blinker-1.9.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting itsdangerous>=2.2.0 (from Flask<4->mlflow==2.16)\n",
      "  Downloading itsdangerous-2.2.0-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: markupsafe>=2.1.1 in /opt/app-root/lib64/python3.11/site-packages (from Flask<4->mlflow==2.16) (3.0.2)\n",
      "Collecting werkzeug>=3.1.0 (from Flask<4->mlflow==2.16)\n",
      "  Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting graphql-core<3.3,>=3.1 (from graphene<4->mlflow==2.16)\n",
      "  Downloading graphql_core-3.2.6-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting graphql-relay<3.3,>=3.1 (from graphene<4->mlflow==2.16)\n",
      "  Downloading graphql_relay-3.2.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: python-dateutil<3,>=2.7.0 in /opt/app-root/lib64/python3.11/site-packages (from graphene<4->mlflow==2.16) (2.9.0.post0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/app-root/lib64/python3.11/site-packages (from matplotlib<4->mlflow==2.16) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/app-root/lib64/python3.11/site-packages (from matplotlib<4->mlflow==2.16) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/app-root/lib64/python3.11/site-packages (from matplotlib<4->mlflow==2.16) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/app-root/lib64/python3.11/site-packages (from matplotlib<4->mlflow==2.16) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in /opt/app-root/lib64/python3.11/site-packages (from matplotlib<4->mlflow==2.16) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/app-root/lib64/python3.11/site-packages (from matplotlib<4->mlflow==2.16) (3.2.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/app-root/lib64/python3.11/site-packages (from pandas<3->mlflow==2.16) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/app-root/lib64/python3.11/site-packages (from pandas<3->mlflow==2.16) (2025.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/app-root/lib64/python3.11/site-packages (from scikit-learn<2->mlflow==2.16) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/app-root/lib64/python3.11/site-packages (from scikit-learn<2->mlflow==2.16) (3.6.0)\n",
      "Collecting greenlet>=1 (from sqlalchemy<3,>=1.4.0->mlflow==2.16)\n",
      "  Downloading greenlet-3.2.3-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: google-auth~=2.0 in /opt/app-root/lib64/python3.11/site-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==2.16.0->mlflow==2.16) (2.38.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/app-root/lib64/python3.11/site-packages (from gitpython<4,>=3.1.9->mlflow-skinny==2.16.0->mlflow==2.16) (4.0.12)\n",
      "Collecting zipp>=3.20 (from importlib-metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==2.16.0->mlflow==2.16)\n",
      "  Downloading zipp-3.23.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.56b0 (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==2.16.0->mlflow==2.16)\n",
      "  Downloading opentelemetry_semantic_conventions-0.56b0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /opt/app-root/lib64/python3.11/site-packages (from python-dateutil<3,>=2.7.0->graphene<4->mlflow==2.16) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/app-root/lib64/python3.11/site-packages (from requests<3,>=2.17.3->mlflow-skinny==2.16.0->mlflow==2.16) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/app-root/lib64/python3.11/site-packages (from requests<3,>=2.17.3->mlflow-skinny==2.16.0->mlflow==2.16) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/app-root/lib64/python3.11/site-packages (from requests<3,>=2.17.3->mlflow-skinny==2.16.0->mlflow==2.16) (2025.1.31)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/app-root/lib64/python3.11/site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==2.16.0->mlflow==2.16) (5.0.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/app-root/lib64/python3.11/site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.16.0->mlflow==2.16) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/app-root/lib64/python3.11/site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.16.0->mlflow==2.16) (4.9)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /opt/app-root/lib64/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.16.0->mlflow==2.16) (0.6.1)\n",
      "Downloading mlflow-2.16.0-py3-none-any.whl (26.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.6/26.6 MB\u001b[0m \u001b[31m204.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mlflow_skinny-2.16.0-py3-none-any.whl (5.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m232.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading alembic-1.16.4-py3-none-any.whl (247 kB)\n",
      "Downloading docker-7.1.0-py3-none-any.whl (147 kB)\n",
      "Downloading flask-3.1.1-py3-none-any.whl (103 kB)\n",
      "Downloading graphene-3.4.3-py2.py3-none-any.whl (114 kB)\n",
      "Downloading gunicorn-23.0.0-py3-none-any.whl (85 kB)\n",
      "Downloading markdown-3.8.2-py3-none-any.whl (106 kB)\n",
      "Downloading pyarrow-17.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (39.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m248.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sqlalchemy-2.0.41-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m265.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading blinker-1.9.0-py3-none-any.whl (8.5 kB)\n",
      "Downloading databricks_sdk-0.59.0-py3-none-any.whl (676 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m676.2/676.2 kB\u001b[0m \u001b[31m293.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading graphql_core-3.2.6-py3-none-any.whl (203 kB)\n",
      "Downloading graphql_relay-3.2.0-py3-none-any.whl (16 kB)\n",
      "Downloading greenlet-3.2.3-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (585 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m585.5/585.5 kB\u001b[0m \u001b[31m580.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading importlib_metadata-8.7.0-py3-none-any.whl (27 kB)\n",
      "Downloading itsdangerous-2.2.0-py3-none-any.whl (16 kB)\n",
      "Downloading opentelemetry_api-1.35.0-py3-none-any.whl (65 kB)\n",
      "Downloading opentelemetry_sdk-1.35.0-py3-none-any.whl (119 kB)\n",
      "Downloading opentelemetry_semantic_conventions-0.56b0-py3-none-any.whl (201 kB)\n",
      "Downloading sqlparse-0.5.3-py3-none-any.whl (44 kB)\n",
      "Downloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "Downloading mako-1.3.10-py3-none-any.whl (78 kB)\n",
      "Downloading zipp-3.23.0-py3-none-any.whl (10 kB)\n",
      "Installing collected packages: zipp, werkzeug, sqlparse, pyarrow, markdown, Mako, itsdangerous, gunicorn, greenlet, graphql-core, blinker, sqlalchemy, importlib-metadata, graphql-relay, Flask, docker, opentelemetry-api, graphene, databricks-sdk, alembic, opentelemetry-semantic-conventions, opentelemetry-sdk, mlflow-skinny, mlflow\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 19.0.1\n",
      "    Uninstalling pyarrow-19.0.1:\n",
      "      Successfully uninstalled pyarrow-19.0.1\n",
      "Successfully installed Flask-3.1.1 Mako-1.3.10 alembic-1.16.4 blinker-1.9.0 databricks-sdk-0.59.0 docker-7.1.0 graphene-3.4.3 graphql-core-3.2.6 graphql-relay-3.2.0 greenlet-3.2.3 gunicorn-23.0.0 importlib-metadata-8.7.0 itsdangerous-2.2.0 markdown-3.8.2 mlflow-2.16.0 mlflow-skinny-2.16.0 opentelemetry-api-1.35.0 opentelemetry-sdk-1.35.0 opentelemetry-semantic-conventions-0.56b0 pyarrow-17.0.0 sqlalchemy-2.0.41 sqlparse-0.5.3 werkzeug-3.1.3 zipp-3.23.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install mlflow==2.16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9ce552d6-fd7e-4135-91fa-4b346716458b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_to_mlflow(\n",
    "    results_json: str,\n",
    "    mlflow_tracking_uri: str,\n",
    "    experiment_name: str\n",
    "):\n",
    "    import json, pandas as pd, mlflow\n",
    "\n",
    "    with open(results_json) as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Flatten numeric metrics dynamically\n",
    "    flat = []\n",
    "    for task, metrics in data.get(\"results\", {}).items():\n",
    "        for key, value in metrics.items():\n",
    "            if key == \"alias\":\n",
    "                continue\n",
    "            try:\n",
    "                numeric = float(value)\n",
    "            except (ValueError, TypeError):\n",
    "                continue\n",
    "            metric, variant = key.split(\",\", 1)\n",
    "            flat.append({\n",
    "                \"task\": task,\n",
    "                \"metric\": metric,\n",
    "                \"variant\": variant,\n",
    "                \"value\": numeric\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(flat)\n",
    "\n",
    "    mlflow.set_tracking_uri(mlflow_tracking_uri)\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "    with mlflow.start_run():\n",
    "        mlflow.log_artifact(results_json, artifact_path=\"lm_eval_json\")\n",
    "\n",
    "        for _, row in df.iterrows():\n",
    "            name = f\"{row['task']}_{row['metric']}_{row['variant']}\"\n",
    "            mlflow.log_metric(name, row[\"value\"])\n",
    "\n",
    "        # save and log a CSV summary\n",
    "        tmp_csv = \"/tmp/lm_eval_metrics.csv\"\n",
    "        df.to_csv(tmp_csv, index=False)\n",
    "        mlflow.log_artifact(tmp_csv, artifact_path=\"lm_eval_metadata\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2b38c012-7939-4f1b-b033-fa8fad246a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/22 01:43:48 INFO mlflow.tracking._tracking_service.client: 🏃 View run crawling-sponge-864 at: http://mlflow-server.mlflow.svc.cluster.local:8080/#/experiments/3/runs/e05faab9afa643c791cfdae82819649f.\n",
      "2025/07/22 01:43:48 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://mlflow-server.mlflow.svc.cluster.local:8080/#/experiments/3.\n"
     ]
    }
   ],
   "source": [
    "log_to_mlflow(\"./results_json/results.json\", \"http://mlflow-server.mlflow.svc.cluster.local:8080\", \"quant-pipeline-new\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
